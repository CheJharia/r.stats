---
title:
author: "cjlortie"
date: "May 2016"
output:
  html_document:
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
#BIOL5081: Biostatistics Intro
The first six weeks of the course will cover data science & fundamental exploratory data analysis in r.  It is a brief introduction to the contemporary open science, best-practice data and biostatistical working tools. The primary goal is exploration of tools and approaches associated with effective, efficient, reproducible biostatistical analyses. Inspirations include [software carpentry](http://software-carpentry.org), [rforcats](http://rforcats.net), and many, more open, online, free I can direct you to as needed. The [description](http://biology.gradstudies.yorku.ca/courses/biol-5081/) provided by the university is a useful starting point in deciding if the Fall 2016 offering meets your specific needs.

use.ful

bio.stats

adventure.time


[Course outline](https://github.com/cjlortie/r.stats/blob/gh-pages/BIOL5081-Fall2016.pdf)


![](./stats.JPG)

###Lesson 1. Data science (DS).
want data. have data. will collect data.
assumption: in this course, you are here to work with data.
data literacy IS data science.


[WhyR introductory lecture](http://www.slideshare.net/cjlortie/whyr)

[The importance of data viz](http://datascienceplus.com/the-importance-of-data-visualization/)

<b> Philosophy of R stats </b>
Statistical thinking: likelihood, error, and effect sizes. Contemporary statisticians embrace and are mindful of these three key concepts in all research, data, and statistical inference examinations.

Modes of inference with your data: using data, what can you infer or do?
1. Data description
2. Likelihood
3. Estimation
4. Baysian inference - weight and include what we know
5. Prediction
6. Hypothesis testing
7. Decision making -balance gains and risks

This set of ideas provide the foundation for many data science and statistical approached to working with evidence within almost every domain of science.

<b>Data viz first and foremost. Case study #1.</b>
```{r, data viz exercise}
#blog post by Fisseha Berhane
library(ggplot2)
library(dplyr)
library(reshape2)

#Create four groups: setA, setB, setC and setD.
setA=select(anscombe, x=x1,y=y1)
setB=select(anscombe, x=x2,y=y2)
setC=select(anscombe, x=x3,y=y3)
setD=select(anscombe, x=x4,y=y4)

#Add a third column which can help us to identify the four groups.
setA$group ='SetA'
setB$group ='SetB'
setC$group ='SetC'
setD$group ='SetD'

#merge the four datasets
all_data=rbind(setA,setB,setC,setD)  # merging all the four data sets
all_data[c(1,13,23,43),]  # showing sample

#compare summary stats
summary_stats =all_data%>%group_by(group)%>%summarize("mean x"=mean(x),
                                       "Sample variance x"=var(x),
                                       "mean y"=round(mean(y),2),
                                       "Sample variance y"=round(var(y),1),
                                       'Correlation between x and y '=round(cor(x,y),2)
                                      )
models = all_data %>% 
      group_by(group) %>%
      do(mod = lm(y ~ x, data = .)) %>%
      do(data.frame(var = names(coef(.$mod)),
                    coef = round(coef(.$mod),2),
                    group = .$group)) %>%
dcast(., group~var, value.var = "coef")

summary_stats_and_linear_fit = cbind(summary_stats, data_frame("Linear regression" =
                                    paste0("y = ",models$"(Intercept)"," + ",models$x,"x")))

summary_stats_and_linear_fit

#data viz instead as first step
ggplot(all_data, aes(x=x,y=y)) +geom_point(shape = 21, colour = "red", fill = "orange", size = 3)+
    ggtitle("Anscombe's data sets")+geom_smooth(method = "lm",se = FALSE,color='blue') + 
    facet_wrap(~group, scales="free")

```
Outcome from stats first, data viz later (tricked), descriptive estimates of data can be deceptive. Draw first, then interpret.


<b>Survey data from class. Case study #2.</b>
```{r, survey}
#load class survey responses from google poll completed in class
survey<-read.csv("biol5081.1.csv")
str(survey) #check data match what we collected

#data viz
hist(survey$r.experience, xlab="experience in R (1 is none, 5 is extensive)", ylab="frequency", main="Likert Scale 1 to 5")
plot(survey$r.experience~survey$discipline, xlab="discipline", ylab="experience in R")
plot(survey$r.studio, ylab="R Studio")
plot(survey$research.data, ylab="Research data")
#observe patterns by checking plots
```
<b>Observations from data viz</b>
We have limited experience in R. Experience in R varies by research discipline. A total of half the respondents have tried R Studio. Most participants will be working with quantitative data in their own research projects.

```{r, test survey data with EDA}
#Now, try some simple summary statistics.
summary(survey)
#Data summary looks reasonable based on plots, mean R experience is < 2
t.test(survey$r.experience, mu=1) #t-test if mean is different from 1
t.test(survey$r.experience, mu=2) #t-test if mean is different from 2
#A one sample t-test confirms we have a bit experience in R.

m1<-glm(r.experience~discipline, family = poisson, data = survey) #test for differenes between disciplines in R experience
m1 #model summary
anova(m1, test="Chisq") #test whether the differences in model are different
#Too little evidence to be significantly different between disciplines.

```

<b> Practical outcomes of R stats useful for competency test</b>
Understand the difference between R and R Studio.
Use scripts or R Markdown files to save all your work.
Be prepared to share you code.
Load data, clean data, visualize data, then and only then begin applying statistics.
Proximately: be able to use and work with dataframes, vectors, functions, and libraries in R.

###Lesson 2. Workflows & Data Wrangling (WDW).
worflows
reproduce. 
openly.

data wrangling
more than half the battle.

![](./beemo.rodeo.png)

PK talk here :)

<b> Philosophy of R stats </b>
Tidy data make your life easier. Data strutures should match intuition and common sense. Data should have logical structure.  Rows are are observations, columns are variables. Tidy data also increase the viability that others can use your data, do better science, reuse science, and help you and your ideas survive and thrive. A workflow should also include the wrangling you did to get your data ready. If you are data are already very clean in a spreadsheet and easily become a literate, logical dataframe, you should still use annotation within the introductory code to explain the meta-data of your data to some extent and what you did pre-R to get it tidy.  The philosophy here is very similar to the data viz lesson forthcoming with two dominant paradigms. Base R code functions, and pipes %>% and the logic embodied within the libraries associated with the the tidyverse. Generally, I prefer the tidyverse because it is more logical and much easier to remember.  It also has some specific functions needed to address common errors in modern data structures with little code needed to fix them up.

<b>Worflow</b>
Suggested example

<b>Data wrangling</b>

<b>Base R</b>
key concepts
aggregate
tapply
sapply 
lappy
subsetting
as.factor
is.numeric
na

<b>tidyverse</b>

[Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

[tidyr](https://cran.r-project.org/web/packages/tidyr/README.html)

<b> Practical outcomes of R stats useful for competency test</b>

###Lesson 3. Visualization in r (VR).
basic plots.
lattice.
ggplot2.
you need to see data. see the trends. explore them using visuals.

[Contemporary data viz for statistical analyses slide deck](http://www.slideshare.net/cjlortie/data-visualization-in-r-65991145)

<b> Philosophy of R stats </b>
Clean simple graphics are powerful tools in statistics (and in scientific communication).  Tufte and others have shaped data scientists and statisticians in developing more libraries, new standards, and assumptions associated with graphical representations of data.  Data viz must highlight the differences, show underlying data structures, and provide insights into the specific research project. R is infinitely customizable in all these respects.  There are at least two major current paradigms (there are more these are the two dominant idea sets).  Base R plots are simple, relatively flexible, and very easy. However, their grammar, i.e their rules of coding are not modern. Ggplot and related libraries invoke a new, formal grammar of graphics that is more logical, more flexible, but divergent from base R code. It is worth the time to understand the differences and know when to use each.

Evolution of plotting in statistics using R in particular went from base-R then onto lattice then to the ggvis universe with the most recent library being ggplot2. Base-R is certainly useful in some contexts as is the lattice and lattice extra library. However, ggplot2 now encompasses all these capacities with a much simpler set of grammar (i.e. rules and order). Nonetheless, you should be able to read base-R code for plots and be able to do some as well. The philosophy or grammar of modern graphics is well articulated and includes the following key principles.

The grammar of graphics
layers
primacy of layers (simple first, then more complex) i.e. you build up your plots
data are mapped to aesthetic attributes and geometric objects
data first then statistics even in plots

Disclaimer: I love the power of qplots.

<b>Data viz case study #1.</b>
```{r, survey data viz}
library(ggplot2)
survey<-read.csv("BIOL5081.1.csv")
str(survey)

plot(survey$r.experience) #hard to tell what is going on

qplot(r.experience, data=survey) #decided to make bins for me and count up)

#so, we know better and instead do a histogram using base graphics
#basic data viz for EDA
hist(survey$r.experience) #better
qplot(r.experience, data=survey, geom="histogram") #same as what is picked for us
qplot(r.experience, data=survey, geom="histogram", binwidth=0.5)

barplot(survey$r.experience) #confusing
qplot(r.experience, data=survey, geom="bar") #what, it is back!

#basic data viz for EDA but for interactions
plot(survey$discipline, survey$r.experience)
qplot(discipline, r.experience, data=survey) #not the same
qplot(discipline, r.experience, data=survey, geom="boxplot")

plot(survey$r.studio~survey$r.experience) #ugly
qplot(r.experience, r.studio, data=survey) #useless
qplot(r.studio, data=survey, weight = r.experience) #sweet new feature here

#ok, so you get it. grammar different, visuals about the same for super quick, simple plots. The grammar hints at the power that awaits though.

#grammar different, simple x or x~y plots about the same

```

<b>Data viz case study #2.</b>
```{r, diamonds are our best friend}
str(diamonds)
#crazy number of observations. We need less. too many riches not always good.
set.seed(1000)
dsmall<-diamonds[sample(nrow(diamonds), 100), ]

plot(dsmall$carat, dsmall$price)
qplot(carat, price, data=dsmall)

#ok no difference
#now let's see what we can do with qplot with a few bits added
#one little argument extra added
qplot(carat, price, data = dsmall, colour = color)
qplot(carat, price, data = dsmall, shape = cut)

#how about using data viz to even more thoroughly explore potential stats we could do.
#qplots - quick plot, thoughful build of layers
qplot(carat, price, data = dsmall, geom = c("point", "smooth"))

#what about trying some stats on this now, at least from a viz philosophy
qplot(color, price / carat, data = dsmall, geom = "boxplot") #can include formulas and methods

#or check for proportional differences
qplot(carat, data = dsmall, geom = "histogram", fill = color) #to see proportions
qplot(carat, data = dsmall, geom = "histogram", weight = price) # weight by a covariate
     
#final idea, how about subsetting with the data right in the code for the plots!
qplot(carat, data = diamonds, facets = color ~ .,
  geom = "histogram", binwidth = 0.1, xlim = c(0, 3)) #to compare between groups

#qplot is so powerful.
#colour, shape and size have meaning in the R code from this library
#layers added for you by qplots

#qplot gets you 2x and one y or one x and 2y so >2 variables at once easily
```

<b>Data viz case study #3.</b>
```{r, ggplot}
#GGPLOT() gives you even more options for adding layers/options
p <- ggplot(mtcars, aes(x = mpg, y = wt))
p + geom_point()

#now play time with this case study.
#try out some geom options and different aesthetics and make some errors.
#prize for the prettiest plots

#displ is car engine size in Liters
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

#so aethetics are one way to add variables in and expand your plotting power
#however facets are another way to make multiple plots BY a factor

#facet wrap is by one variable
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

#facet_wrap(~cell) - univariate: create a 1-d strip of panels, based on one factor, and wrap the strip into a 2-d matrix
#facet_grid(row~col) - (usually) bivariate: create a 2-d matrix of panels, based on two factors

#facet grid is by two variables
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

#another example more perfect code
p <- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point()
p + facet_wrap(~cyl)

#or just use facets in qplot but much simpler
qplot(displ, hwy, data=mpg, facets = . ~ year) + geom_smooth()

```

<b>Data viz case study #4.</b>
```{r, worked example}
#try it with ecological.footprints.csv


```



<b> Practical outcomes of R stats useful for competency test</b>


###Lesson 4. Exploratory data analysis (EDA) in r.
fundamental descriptive stats.
GLM. GLMM. Post hoc tests.

###Lesson 5. Wrangle, visualize, and analyze.
A graduate-level dataset.
Apply your new mathemagical skills from scratch.
A single three-hour block.
As advanced as you want to be.
Fundamental principles demonstrated.
At least two fundamental statistical tests demonstrated.

###Lesson 6. Competency test.
deliberate practice.
tested with feedback.
a virtual repeat of last week but evaluated



