---
title:
author: "cjlortie"
date: "May 2016"
output:
  html_document:
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
#BIOL5081: Biostatistics Intro
The first six weeks of the course will cover data science & fundamental exploratory data analysis in r.  It is a brief introduction to the contemporary open science, best-practice data and biostatistical working tools. The primary goal is exploration of tools and approaches associated with effective, efficient, reproducible biostatistical analyses. Inspirations include [software carpentry](http://software-carpentry.org), [rforcats](http://rforcats.net), and many, more open, online, free I can direct you to as needed. The [description](http://biology.gradstudies.yorku.ca/courses/biol-5081/) provided by the university is a useful starting point in deciding if the Fall 2016 offering meets your specific needs.

use.ful

bio.stats

adventure.time


[Course outline](https://github.com/cjlortie/r.stats/blob/gh-pages/BIOL5081-Fall2016.pdf)


![](./stats.JPG)

###Lesson 1. Data science (DS).
want data. have data. will collect data.
assumption: in this course, you are here to work with data.
data literacy IS data science.


[WhyR introductory lecture](http://www.slideshare.net/cjlortie/whyr)

[The importance of data viz](http://datascienceplus.com/the-importance-of-data-visualization/)

<b> Philosophy of R stats </b>
Statistical thinking: likelihood, error, and effect sizes. Contemporary statisticians embrace and are mindful of these three key concepts in all research, data, and statistical inference examinations.

Modes of inference with your data: using data, what can you infer or do?
1. Data description
2. Likelihood
3. Estimation
4. Baysian inference - weight and include what we know
5. Prediction
6. Hypothesis testing
7. Decision making -balance gains and risks

This set of ideas provide the foundation for many data science and statistical approached to working with evidence within almost every domain of science.

<b>Data viz first and foremost. Case study #1.</b>
```{r, data viz exercise}
#blog post by Fisseha Berhane
library(ggplot2)
library(dplyr)
library(reshape2)

#Create four groups: setA, setB, setC and setD.
setA=select(anscombe, x=x1,y=y1)
setB=select(anscombe, x=x2,y=y2)
setC=select(anscombe, x=x3,y=y3)
setD=select(anscombe, x=x4,y=y4)

#Add a third column which can help us to identify the four groups.
setA$group ='SetA'
setB$group ='SetB'
setC$group ='SetC'
setD$group ='SetD'

#merge the four datasets
all_data=rbind(setA,setB,setC,setD)  # merging all the four data sets
all_data[c(1,13,23,43),]  # showing sample

#compare summary stats
summary_stats =all_data%>%group_by(group)%>%summarize("mean x"=mean(x),
                                       "Sample variance x"=var(x),
                                       "mean y"=round(mean(y),2),
                                       "Sample variance y"=round(var(y),1),
                                       'Correlation between x and y '=round(cor(x,y),2)
                                      )
models = all_data %>% 
      group_by(group) %>%
      do(mod = lm(y ~ x, data = .)) %>%
      do(data.frame(var = names(coef(.$mod)),
                    coef = round(coef(.$mod),2),
                    group = .$group)) %>%
dcast(., group~var, value.var = "coef")

summary_stats_and_linear_fit = cbind(summary_stats, data_frame("Linear regression" =
                                    paste0("y = ",models$"(Intercept)"," + ",models$x,"x")))

summary_stats_and_linear_fit

#data viz instead as first step
ggplot(all_data, aes(x=x,y=y)) +geom_point(shape = 21, colour = "red", fill = "orange", size = 3)+
    ggtitle("Anscombe's data sets")+geom_smooth(method = "lm",se = FALSE,color='blue') + 
    facet_wrap(~group, scales="free")

```
Outcome from stats first, data viz later (tricked), descriptive estimates of data can be deceptive. Draw first, then interpret.


<b>Survey data from class. Case study #2.</b>
```{r, survey}
#load class survey responses from google poll completed in class
survey<-read.csv("biol5081.1.csv")
str(survey) #check data match what we collected

#data viz
hist(survey$r.experience, xlab="experience in R (1 is none, 5 is extensive)", ylab="frequency", main="Likert Scale 1 to 5")
plot(survey$r.experience~survey$discipline, xlab="discipline", ylab="experience in R")
plot(survey$r.studio, ylab="R Studio")
plot(survey$research.data, ylab="Research data")
#observe patterns by checking plots
```
<b>Observations from data viz</b>
We have limited experience in R. Experience in R varies by research discipline. A total of half the respondents have tried R Studio. Most participants will be working with quantitative data in their own research projects.

```{r, test survey data with EDA}
#Now, try some simple summary statistics.
summary(survey)
#Data summary looks reasonable based on plots, mean R experience is < 2
t.test(survey$r.experience, mu=1) #t-test if mean is different from 1
t.test(survey$r.experience, mu=2) #t-test if mean is different from 2
#A one sample t-test confirms we have a bit experience in R.

m1<-glm(r.experience~discipline, family = poisson, data = survey) #test for differenes between disciplines in R experience
m1 #model summary
anova(m1, test="Chisq") #test whether the differences in model are different
#Too little evidence to be significantly different between disciplines.

```

<b> Practical outcomes of R stats useful for competency test</b>
Understand the difference between R and R Studio.
Use scripts or R Markdown files to save all your work.
Be prepared to share you code.
Load data, clean data, visualize data, then and only then begin applying statistics.
Proximately: be able to use and work with dataframes, vectors, functions, and libraries in R.

###Lesson 2. Workflows & Data Wrangling (WDW).
worflows
reproduce. 
openly.

data wrangling
more than half the battle.

![](./beemo.rodeo.png)

PK talk here :)

<b> Philosophy of R stats </b>
Tidy data make your life easier. Data strutures should match intuition and common sense. Data should have logical structure.  Rows are are observations, columns are variables. Tidy data also increase the viability that others can use your data, do better science, reuse science, and help you and your ideas survive and thrive. A workflow should also include the wrangling you did to get your data ready. If you are data are already very clean in a spreadsheet and easily become a literate, logical dataframe, you should still use annotation within the introductory code to explain the meta-data of your data to some extent and what you did pre-R to get it tidy.  The philosophy here is very similar to the data viz lesson forthcoming with two dominant paradigms. Base R code functions, and pipes %>% and the logic embodied within the libraries associated with the the tidyverse. Generally, I prefer the tidyverse because it is more logical and much easier to remember.  It also has some specific functions needed to address common errors in modern data structures with little code needed to fix them up.

<b>Worflow</b>
Suggested example

<b>Data wrangling</b>

<b>Base R</b>
key concepts
aggregate
tapply
sapply 
lappy
subsetting
as.factor
is.numeric
na

<b>tidyverse: primarily dyplyr</b>
[Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

tidyr 
[tidyr](https://cran.r-project.org/web/packages/tidyr/README.html)

<b> Practical outcomes of R stats useful for competency test</b>

###Lesson 3. Visualization in r (VR).
basic plots.
lattice.
ggplot2.
you need to see data. see the trends. explore them using visuals.

PK talk here :)

<b> Philosophy of R stats </b>
Clean simple graphics are powerful tools in statistics (and in scientific communication).  Tufte and others have shaped data scientists and statisticians in developing more libraries, new standards, and assumptions associated with graphical representations of data.  Data viz must highlight the differences, show underlying data structures, and provide insights into the specific research project. R is infinitely customizable in all these respects.  There are at least two major current paradigms (there are more these are the two dominant idea sets).  Base R plots are simple, relatively flexible, and very easy. However, their grammar, i.e their rules of coding are not modern. Ggplot and related libraries invoke a new, formal grammar of graphics that is more logical, more flexible, but divergent from base R code. It is worth the time to understand the differences and know when to use each.

<b>Base R</b>

<b>GGPLOT2</b>
qplots

ggplots

<b> Practical outcomes of R stats useful for competency test</b>


###Lesson 4. Exploratory data analysis (EDA) in r.
fundamental descriptive stats.
GLM. GLMM. Post hoc tests.

###Lesson 5. Wrangle, visualize, and analyze.
A graduate-level dataset.
Apply your new mathemagical skills from scratch.
A single three-hour block.
As advanced as you want to be.
Fundamental principles demonstrated.
At least two fundamental statistical tests demonstrated.

###Lesson 6. Competency test.
deliberate practice.
tested with feedback.
a virtual repeat of last week but evaluated



